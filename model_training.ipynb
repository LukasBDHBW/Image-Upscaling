{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8702,"status":"ok","timestamp":1687427997018,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"Evh_r8dmRavT"},"outputs":[],"source":["from PIL import Image\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms, datasets\n","import os\n","from torch.utils.data import DataLoader, Dataset\n","import math\n","from torchvision.transforms import ColorJitter, Normalize\n","from torch.utils.data import ConcatDataset\n","from torch.utils.data import Subset\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3720,"status":"ok","timestamp":1687428000723,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"Ruw8dg8LZVCQ","outputId":"8fe2cf75-d18a-4647-c164-f8977079a1f6"},"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 3] Das System kann den angegebenen Pfad nicht finden: '/kaggle/input/animal-faces/afhq/train/dog'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m output_size \u001b[39m=\u001b[39m (\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Iterate over the files in the folder\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfor\u001b[39;00m number, filename \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(folder_path)):\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m   \u001b[39m# Construct the full path to the image file\u001b[39;00m\n\u001b[0;32m     26\u001b[0m   image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, filename)\n\u001b[0;32m     28\u001b[0m   \u001b[39m# Apply downsampling to the image\u001b[39;00m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: '/kaggle/input/animal-faces/afhq/train/dog'"]}],"source":["def downsample_image(image_path, output_size):\n","    # Open the image\n","    image = Image.open(image_path)\n","\n","    # Perform downsampling using bicubic interpolation\n","    downscaled_image = image.resize(output_size, resample=Image.BICUBIC)\n","\n","    return downscaled_image\n","\n","# Folder path containing the images\n","folder_path = \"/kaggle/input/animal-faces/afhq/train/dog\"\n","\n","# Defining folder for downscaled images serving for input for modelling (&upscaling)\n","output_folder_path = \"/kaggle/working/dog_low_res\"\n","if not os.path.exists(output_folder_path):\n","    os.makedirs(output_folder_path)\n","\n","\n","# Output size for downsampling (by a factor of 3)\n","output_size = (128, 128)\n","\n","# Iterate over the files in the folder\n","for number, filename in enumerate(os.listdir(folder_path)):\n","\n","  # Construct the full path to the image file\n","  image_path = os.path.join(folder_path, filename)\n","\n","  # Apply downsampling to the image\n","  downsampled_image = downsample_image(image_path, output_size)\n","\n","  # Save the downscaled image\n","  output_filename = f\"downsampled_{filename}\"\n","  output_path = os.path.join(output_folder_path, output_filename)\n","  downsampled_image.save(output_path)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1687428000724,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"2QU9qK3GN3ws"},"outputs":[],"source":["def downsample_image(image_path, output_size):\n","    # Open the image\n","    image = Image.open(image_path)\n","\n","    # Perform downsampling using bicubic interpolation\n","    downscaled_image = image.resize(output_size, resample=Image.BICUBIC)\n","\n","    return downscaled_image\n","\n","# Folder path containing the images\n","folder_path = \"./data/data_final\"\n","\n","# Defining folder for downscaled images serving for input for modelling (&upscaling)\n","output_folder_path = \"./data/mensch_low_res\"\n","\n","test_low_res_folder = \"./data/mensch_test_low_res\"\n","test_original = \"./data/mensch_test_original\"\n","\n","if not os.path.exists(output_folder_path):\n","    os.makedirs(output_folder_path)\n","if not os.path.exists(test_low_res_folder):\n","    os.makedirs(test_low_res_folder)\n","if not os.path.exists(test_original):\n","    os.makedirs(test_original)\n","\n","\n","# Output size for downsampling (by a factor of 3)\n","output_size = (128, 128)\n","\n","number_images = len(os.listdir(folder_path))\n","\n","# Iterate over the files in the folder\n","for number, filename in enumerate(os.listdir(folder_path)):\n","\n","    # Check if the file is an image (optional)\n","    if number<=0.8*number_images:\n","      # Construct the full path to the image file\n","      image_path = os.path.join(folder_path, filename)\n","\n","      # Apply downsampling to the image\n","      downsampled_image = downsample_image(image_path, output_size)\n","\n","      # Save the downscaled image\n","      output_filename = f\"downsampled_{filename}\"\n","      output_path = os.path.join(output_folder_path, output_filename)\n","      downsampled_image.save(output_path)\n","\n","    else:\n","      # Construct the full path to the image file\n","      image_path = os.path.join(folder_path, filename)\n","\n","      image = Image.open(image_path)\n","      image.save(os.path.join(test_original, filename))\n","\n","\n","      # Apply downsampling to the image\n","      downsampled_image = downsample_image(image_path, output_size)\n","\n","      # Save the downscaled image\n","      output_filename = f\"downsampled_{filename}\"\n","      output_path = os.path.join(test_low_res_folder, output_filename)\n","      downsampled_image.save(output_path)\n","\n","      os.remove(os.path.join(folder_path, filename))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VrjMPPdsa8zL"},"source":["### data loading (with augmentation)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"elapsed":358973,"status":"error","timestamp":1687428359694,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"Hmwoaegma5-b","outputId":"64f003ce-dd79-461d-ce76-d96e392d1236"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-7f469c149e40>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0maugmented_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0maugmented_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_color_jitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_color_jitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0maugmented_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhue_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rgb2hsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36m_rgb2hsv\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# Implementation is based on https://github.com/python-pillow/Pillow/blob/4174d4267616897df3746d315d5a2d0f82c656ee/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# src/libImaging/Convert.c#L330\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mmaxc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0mminc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# low_res_folder = \"./drive/MyDrive/dog_low_res\"\n","# high_res_folder = \"./drive/MyDrive/dog\"\n","\n","\n","# # === creating dataset with all images ===\n","# class CustomDataset(Dataset):\n","#     def __init__(self, low_res_folder, high_res_folder, transform=None):\n","#         self.low_res_images = sorted(os.listdir(low_res_folder))\n","#         self.high_res_images = sorted(os.listdir(high_res_folder))\n","#         self.transform = transform\n","\n","#     def __len__(self):\n","#         return len(self.low_res_images)\n","\n","#     def __getitem__(self, index):\n","#         low_res_image = Image.open(os.path.join(low_res_folder, self.low_res_images[index]))\n","#         high_res_image = Image.open(os.path.join(high_res_folder, self.high_res_images[index]))\n","\n","#         if self.transform is not None:\n","#             low_res_image = self.transform(low_res_image)\n","#             high_res_image = self.transform(high_res_image)\n","\n","#         return low_res_image, high_res_image\n","\n","# # transform to tensor\n","# base_transform = transforms.Compose([\n","#     transforms.ToTensor()\n","# ])\n","\n","# # original dataset\n","# dataset = CustomDataset(low_res_folder, high_res_folder, transform=base_transform)\n","\n","\n","\n","# # === Splitting into train and val sets ===\n","\n","# train_size = 0.8  # Proportion of data to be used for training\n","# dataset_size = len(dataset)\n","# split = int(train_size * dataset_size)\n","# train_indices = list(range(split))\n","# val_indices = list(range(split, dataset_size))\n","\n","# # Create train dataset as a subset of the combined dataset\n","# train_dataset = Subset(dataset, train_indices)\n","\n","# # Create val dataset as a subset of the combined dataset\n","# val_dataset = Subset(dataset, val_indices)\n","\n","\n","\n","# # === Normalization ===\n","# normalize = Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","\n","# train_dataset = [(normalize(image), normalize(target)) for image, target in train_dataset]\n","\n","# val_dataset = [(normalize(image), normalize(target)) for image, target in val_dataset]\n","\n","\n","\n","# # === train_data augmentation ===\n","\n","# # color jitter augmentation for training\n","# train_color_jitter = ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n","\n","# # augmentation factor\n","# augmentation_factor = 2\n","\n","# # augmented datasets with random color jitter\n","# augmented_datasets = []\n","# for _ in range(augmentation_factor):\n","#     augmented_dataset = []\n","#     for image, target in train_dataset:\n","#         augmented_dataset.append((train_color_jitter(image), train_color_jitter(target)))\n","#     augmented_datasets.append(augmented_dataset)\n","\n","# # combine original and augmented datasets\n","# combined_datasets = [train_dataset] + augmented_datasets\n","# combined_dataset = ConcatDataset(combined_datasets)\n","\n","\n","\n","# # === final data loaders ===\n","\n","# # Data loaders for train and val sets\n","# batch_size = 32\n","# train_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n","# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# # Number of samples in each set\n","# print(f\"Number of training samples originally: {len(train_dataset)}, now augmented to: {len(combined_dataset)}\")\n","# print(f\"Number of val samples: {len(val_dataset)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["low_res_folder = \"/kaggle/working/dog_low_res\"\n","high_res_folder = \"/kaggle/input/animal-faces/afhq/train/dog\"\n","\n","\n","# === creating dataset with all images ===\n","class CustomDataset(Dataset):\n","    def __init__(self, low_res_folder, high_res_folder, transform=None):\n","        self.low_res_folder = low_res_folder\n","        self.high_res_folder = high_res_folder\n","        self.low_res_images = sorted(os.listdir(low_res_folder))\n","        self.high_res_images = sorted(os.listdir(high_res_folder))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.low_res_images)\n","\n","    def __getitem__(self, index):\n","        low_res_image = Image.open(os.path.join(self.low_res_folder, self.low_res_images[index]))\n","        high_res_image = Image.open(os.path.join(self.high_res_folder, self.high_res_images[index]))\n","\n","        if self.transform is not None:\n","            low_res_image = self.transform(low_res_image)\n","            high_res_image = self.transform(high_res_image)\n","\n","        return low_res_image, high_res_image\n","\n","# transform to tensor & normalize\n","normalize = Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","\n","base_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    normalize\n","])\n","\n","# original dataset\n","dataset = CustomDataset(low_res_folder, high_res_folder, transform=base_transform)\n","\n","\n","\n","# === Splitting into train and val sets ===\n","\n","train_size = 0.8  # Proportion of data to be used for training\n","dataset_size = len(dataset)\n","split = int(train_size * dataset_size)\n","train_indices = list(range(split))\n","val_indices = list(range(split, dataset_size))\n","\n","# Create train dataset as a subset of the combined dataset\n","train_dataset = Subset(dataset, train_indices)\n","\n","# Create val dataset as a subset of the combined dataset\n","val_dataset = Subset(dataset, val_indices)\n","\n","\n","\n","# === final data loaders ===\n","\n","# Data loaders for train and val sets\n","batch_size = 128\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Number of samples in each set\n","print(f\"Number of training samples originally: {len(train_dataset)}\")\n","print(f\"Number of val samples: {len(val_dataset)}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eRnCPEdja_mT"},"source":["### SRCNN"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":303652,"status":"ok","timestamp":1687426760337,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"0gzlOYjOZVy8","outputId":"985020d4-02da-4d2d-fa4e-06dbf6ef8801"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Loss: 0.1283, PSNR: 8.916304823903484\n","Loss (validation): 0.1736, PSNR (validation): 7.604106464489315\n","Epoch [2/10], Loss: 0.1220, PSNR: 9.137848423008013\n","Loss (validation): 0.1720, PSNR (validation): 7.645366720205501\n","Epoch [3/10], Loss: 0.0960, PSNR: 10.175107758089872\n","Loss (validation): 0.1720, PSNR (validation): 7.644895610697584\n","Epoch [4/10], Loss: 0.0786, PSNR: 11.04723886717254\n","Loss (validation): 0.1713, PSNR (validation): 7.663384318469921\n","Epoch [5/10], Loss: 0.0614, PSNR: 12.117327232179981\n","Loss (validation): 0.1710, PSNR (validation): 7.6689281909895435\n","Epoch [6/10], Loss: 0.1112, PSNR: 9.537440829689723\n","Loss (validation): 0.1712, PSNR (validation): 7.6646840091797594\n","Epoch [7/10], Loss: 0.1348, PSNR: 8.703944118333364\n","Loss (validation): 0.1722, PSNR (validation): 7.63884046002568\n","Epoch [8/10], Loss: 0.0904, PSNR: 10.438977789088515\n","Loss (validation): 0.1715, PSNR (validation): 7.657047145614541\n","Epoch [9/10], Loss: 0.0899, PSNR: 10.462438624621978\n","Loss (validation): 0.1710, PSNR (validation): 7.670334757039252\n","Epoch [10/10], Loss: 0.0677, PSNR: 11.693491150804247\n","Loss (validation): 0.1708, PSNR (validation): 7.675813618203781\n"]}],"source":["# SRCNN model\n","class SRCNN(nn.Module):\n","    def __init__(self):\n","        super(SRCNN, self).__init__()\n","        self.interpolation = nn.Upsample(scale_factor=4, mode='bicubic')\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0)\n","        self.relu2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, stride=1, padding=2)\n","        self.relu3 = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.interpolation(x)\n","        x = self.relu1(self.conv1(x))\n","        x = self.relu2(self.conv2(x))\n","        x = self.relu3(self.conv3(x))\n","        return x\n","\n","# Training hardware\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# instance of the CNN model\n","model = SRCNN().to(device)\n","\n","# hyperparameters\n","learning_rate = 0.001\n","num_epochs = 10\n","\n","# loss function and optimizer\n","criterion = nn.MSELoss() # note: standard MSE is used, PSNR normally not used for training (just as metric at the end)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training process\n","for epoch in range(num_epochs):\n","    for input_data, desired_data in train_loader:\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Print training loss per epoch\n","    psnr = 10 * math.log10(1 / loss.item())\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, PSNR: {psnr}\")\n","\n","    # Validating the model (on validation data)\n","    for input_data, desired_data in val_loader:\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","\n","    # Print training loss per epoch\n","    psnr = 10 * math.log10(1 / loss.item())\n","\n","    print(f\"Loss (validation): {loss.item():.4f}, PSNR (validation): {psnr}\")\n","\n","# saving the model\n","torch.save(model.state_dict(), \"SRCNN.pth\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IIwglB9obBWf"},"source":["### FSRCNN"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241593,"status":"ok","timestamp":1687427178503,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"61XUL-tBbCVo","outputId":"9314a921-3999-427f-8208-8816e981bb22"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/40], Loss: 0.0323, PSNR: 14.905335504968562\n","Loss (validation): 0.0491, PSNR (validation): 13.092966777874882\n","Epoch [2/40], Loss: 0.0352, PSNR: 14.540321559611806\n","Loss (validation): 0.0446, PSNR (validation): 13.502632291955203\n","Epoch [3/40], Loss: 0.0283, PSNR: 15.480010865330476\n","Loss (validation): 0.0418, PSNR (validation): 13.783512723062803\n","Epoch [4/40], Loss: 0.0294, PSNR: 15.31094059723694\n","Loss (validation): 0.0383, PSNR (validation): 14.170864947352143\n","Epoch [5/40], Loss: 0.0312, PSNR: 15.065089931378733\n","Loss (validation): 0.0370, PSNR (validation): 14.323281188779323\n","Epoch [6/40], Loss: 0.0294, PSNR: 15.313538459650978\n","Loss (validation): 0.0346, PSNR (validation): 14.607680699022245\n","Epoch [7/40], Loss: 0.0150, PSNR: 18.234631743846535\n","Loss (validation): 0.0245, PSNR (validation): 16.10089254596251\n","Epoch [8/40], Loss: 0.0143, PSNR: 18.455967986404303\n","Loss (validation): 0.0232, PSNR (validation): 16.352925433769503\n","Epoch [9/40], Loss: 0.0205, PSNR: 16.891415753667722\n","Loss (validation): 0.0221, PSNR (validation): 16.552287587290493\n","Epoch [10/40], Loss: 0.0150, PSNR: 18.239939669924585\n","Loss (validation): 0.0218, PSNR (validation): 16.60706614297971\n","Epoch [11/40], Loss: 0.0202, PSNR: 16.95379587759481\n","Loss (validation): 0.0225, PSNR (validation): 16.486869524519793\n","Epoch [12/40], Loss: 0.0211, PSNR: 16.764680210476033\n","Loss (validation): 0.0210, PSNR (validation): 16.780974289255095\n","Epoch [13/40], Loss: 0.0152, PSNR: 18.18096829581225\n","Loss (validation): 0.0215, PSNR (validation): 16.66631567504972\n","Epoch [14/40], Loss: 0.0224, PSNR: 16.504000991985677\n","Loss (validation): 0.0203, PSNR (validation): 16.92345252785074\n","Epoch [15/40], Loss: 0.0165, PSNR: 17.837678881065273\n","Loss (validation): 0.0202, PSNR (validation): 16.956632851097805\n","Epoch [16/40], Loss: 0.0137, PSNR: 18.64228163718417\n","Loss (validation): 0.0204, PSNR (validation): 16.893918677439498\n","Epoch [17/40], Loss: 0.0200, PSNR: 16.981106417537497\n","Loss (validation): 0.0198, PSNR (validation): 17.04245738619893\n","Epoch [18/40], Loss: 0.0142, PSNR: 18.48018359395531\n","Loss (validation): 0.0197, PSNR (validation): 17.048504099939514\n","Epoch [19/40], Loss: 0.0201, PSNR: 16.972883402414993\n","Loss (validation): 0.0196, PSNR (validation): 17.082224087797822\n","Epoch [20/40], Loss: 0.0119, PSNR: 19.249559139248074\n","Loss (validation): 0.0193, PSNR (validation): 17.152558924346145\n","Epoch [21/40], Loss: 0.0136, PSNR: 18.67115181041703\n","Loss (validation): 0.0189, PSNR (validation): 17.24380358746655\n","Epoch [22/40], Loss: 0.0217, PSNR: 16.635244899542915\n","Loss (validation): 0.0197, PSNR (validation): 17.05181391602335\n","Epoch [23/40], Loss: 0.0156, PSNR: 18.060782540416753\n","Loss (validation): 0.0187, PSNR (validation): 17.27471602664926\n","Epoch [24/40], Loss: 0.0129, PSNR: 18.880950815081505\n","Loss (validation): 0.0192, PSNR (validation): 17.158991456986524\n","Epoch [25/40], Loss: 0.0171, PSNR: 17.671719346898083\n","Loss (validation): 0.0185, PSNR (validation): 17.326184304737566\n","Epoch [26/40], Loss: 0.0143, PSNR: 18.447153891418324\n","Loss (validation): 0.0185, PSNR (validation): 17.336100958892267\n","Epoch [27/40], Loss: 0.0137, PSNR: 18.632794915593976\n","Loss (validation): 0.0191, PSNR (validation): 17.179065021822648\n","Epoch [28/40], Loss: 0.0182, PSNR: 17.391247135051824\n","Loss (validation): 0.0187, PSNR (validation): 17.28191922554619\n","Epoch [29/40], Loss: 0.0138, PSNR: 18.604386009814867\n","Loss (validation): 0.0180, PSNR (validation): 17.452672297290373\n","Epoch [30/40], Loss: 0.0166, PSNR: 17.804159968606058\n","Loss (validation): 0.0188, PSNR (validation): 17.26274378534613\n","Epoch [31/40], Loss: 0.0170, PSNR: 17.703572179938803\n","Loss (validation): 0.0183, PSNR (validation): 17.37903514604054\n","Epoch [32/40], Loss: 0.0190, PSNR: 17.220755976649315\n","Loss (validation): 0.0178, PSNR (validation): 17.491983852516498\n","Epoch [33/40], Loss: 0.0108, PSNR: 19.662762439136948\n","Loss (validation): 0.0180, PSNR (validation): 17.440000353934103\n","Epoch [34/40], Loss: 0.0164, PSNR: 17.845306611806183\n","Loss (validation): 0.0181, PSNR (validation): 17.431222731056717\n","Epoch [35/40], Loss: 0.0135, PSNR: 18.688185037076217\n","Loss (validation): 0.0179, PSNR (validation): 17.469205405125255\n","Epoch [36/40], Loss: 0.0141, PSNR: 18.51037663417367\n","Loss (validation): 0.0179, PSNR (validation): 17.47507494179052\n","Epoch [37/40], Loss: 0.0137, PSNR: 18.63304616558532\n","Loss (validation): 0.0199, PSNR (validation): 17.006229005824764\n","Epoch [38/40], Loss: 0.0166, PSNR: 17.787095098952765\n","Loss (validation): 0.0155, PSNR (validation): 18.10493999837723\n","Epoch [39/40], Loss: 0.0116, PSNR: 19.36513966392577\n","Loss (validation): 0.0146, PSNR (validation): 18.343308136863705\n","Epoch [40/40], Loss: 0.0118, PSNR: 19.265509217353905\n","Loss (validation): 0.0148, PSNR (validation): 18.297844728319838\n"]}],"source":["class FSRCNN(nn.Module):\n","    def __init__(self, d=56, s=12, m=4):\n","        super(FSRCNN, self).__init__()\n","        # Feature Extraction\n","        self.conv1 = nn.Conv2d(3, d, kernel_size=5, padding=2)\n","        self.relu1 = nn.PReLU(d)\n","        # Shrinking\n","        self.conv2 = nn.Conv2d(d, s, kernel_size=1)\n","        self.relu2 = nn.PReLU(s)\n","        # Non-linear Mapping\n","        self.mapping = nn.Sequential(*[nn.Sequential(\n","            nn.Conv2d(s, s, kernel_size=3, padding=1),\n","            nn.PReLU(s)\n","        ) for _ in range(m)])\n","        # Expanding\n","        self.conv3 = nn.Conv2d(s, d, kernel_size=1)\n","        self.relu3 = nn.PReLU(d)\n","        # Deconvolution\n","        self.deconv = nn.ConvTranspose2d(d, 3, kernel_size=9, stride=4, padding=4, output_padding=3)\n","\n","    def forward(self, x):\n","        x = self.relu1(self.conv1(x))\n","        x = self.relu2(self.conv2(x))\n","        x = self.mapping(x)\n","        x = self.relu3(self.conv3(x))\n","        x = self.deconv(x)\n","        return x\n","\n","# Training hardware\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# instance of the CNN model\n","model = FSRCNN().to(device)\n","\n","# hyperparameters\n","learning_rate = 0.001\n","num_epochs = 40\n","\n","# loss function and optimizer\n","criterion = nn.MSELoss() # note: standard MSE is used, PSNR normally not used for training (just as metric at the end)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training process\n","for epoch in range(num_epochs):\n","    for input_data, desired_data in train_loader:\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Print training loss per epoch\n","    psnr = 10 * math.log10(1 / loss.item())\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, PSNR: {psnr}\")\n","\n","    # Validating the model (on validation data)\n","    val_loss = 0\n","    number_batches = 0\n","    for input_data, desired_data in val_loader:\n","        number_batches += 1\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","        val_loss += loss.item()\n","    \n","    val_loss_avg = val_loss / number_batches   \n","\n","    # Print training loss per epoch\n","    psnr = 10 * math.log10(1 / val_loss_avg)\n","\n","    print(f\"Loss (validation): {val_loss_avg:.4f}, PSNR (validation): {psnr}\")\n","\n","\n","# saving the model\n","torch.save(model.state_dict(), \"FSRCNN.pth\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yrLNqyDyJt2P"},"source":["### Bicubic"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1074,"status":"ok","timestamp":1687427876086,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"An0Ri4gtJZbC","outputId":"b5d806d6-c5f7-4dbe-f07a-69012eb94eef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss (validation): 0.0114, PSNR (validation): 19.41890646298794\n"]}],"source":["class bicubic(nn.Module):\n","    def __init__(self):\n","        super(bicubic, self).__init__()\n","        self.interpolation = nn.Upsample(scale_factor=4, mode='bicubic')\n","\n","    def forward(self, x):\n","        x = self.interpolation(x)\n","        return x\n","\n","# Training hardware\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# instance of the CNN model\n","model = bicubic().to(device)\n","\n","# loss function and optimizer\n","criterion = nn.MSELoss() # note: standard MSE is used, PSNR normally not used for training (just as metric at the end)\n","\n","# Validating the model (on validation data)\n","for input_data, desired_data in val_loader:\n","    # Move input and desired images to device\n","    input_data = input_data.to(device)\n","    desired_data = desired_data.to(device)\n","\n","    # Forward pass\n","    output_images = model(input_data)\n","\n","    # Calculate loss\n","    loss = criterion(output_images, desired_data)\n","\n","# Print training loss per epoch\n","psnr = 10 * math.log10(1 / loss.item())\n","\n","print(f\"Loss (validation): {loss.item():.4f}, PSNR (validation): {psnr}\")\n","\n","\n","# saving the model\n","torch.save(model.state_dict(), \"bicubic.pth\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5s9yDnLZLDk5"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOZYggG0vihTHkmSNt2+iCH","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
