{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":8702,"status":"ok","timestamp":1687427997018,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"Evh_r8dmRavT"},"outputs":[],"source":["from PIL import Image\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms, datasets\n","import os\n","from torch.utils.data import DataLoader, Dataset\n","import math\n","from torchvision.transforms import ColorJitter, Normalize\n","from torch.utils.data import ConcatDataset\n","from torch.utils.data import Subset\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3720,"status":"ok","timestamp":1687428000723,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"Ruw8dg8LZVCQ","outputId":"8fe2cf75-d18a-4647-c164-f8977079a1f6"},"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 3] Das System kann den angegebenen Pfad nicht finden: '/kaggle/input/animal-faces/afhq/train/dog'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m output_size \u001b[39m=\u001b[39m (\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Iterate over the files in the folder\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfor\u001b[39;00m number, filename \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(folder_path)):\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m   \u001b[39m# Construct the full path to the image file\u001b[39;00m\n\u001b[0;32m     26\u001b[0m   image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, filename)\n\u001b[0;32m     28\u001b[0m   \u001b[39m# Apply downsampling to the image\u001b[39;00m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: '/kaggle/input/animal-faces/afhq/train/dog'"]}],"source":["def downsample_image(image_path, output_size):\n","    # Open the image\n","    image = Image.open(image_path)\n","\n","    # Perform downsampling using bicubic interpolation\n","    downscaled_image = image.resize(output_size, resample=Image.BICUBIC)\n","\n","    return downscaled_image\n","\n","# Folder path containing the images\n","folder_path = \"/kaggle/input/animal-faces/afhq/train/dog\"\n","\n","# Defining folder for downscaled images serving for input for modelling (&upscaling)\n","output_folder_path = \"/kaggle/working/dog_low_res\"\n","if not os.path.exists(output_folder_path):\n","    os.makedirs(output_folder_path)\n","\n","\n","# Output size for downsampling (by a factor of 3)\n","output_size = (128, 128)\n","\n","# Iterate over the files in the folder\n","for number, filename in enumerate(os.listdir(folder_path)):\n","\n","  # Construct the full path to the image file\n","  image_path = os.path.join(folder_path, filename)\n","\n","  # Apply downsampling to the image\n","  downsampled_image = downsample_image(image_path, output_size)\n","\n","  # Save the downscaled image\n","  output_filename = f\"downsampled_{filename}\"\n","  output_path = os.path.join(output_folder_path, output_filename)\n","  downsampled_image.save(output_path)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1687428000724,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"2QU9qK3GN3ws"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m   output_filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdownsampled_\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m   output_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_folder_path, output_filename)\n\u001b[1;32m---> 46\u001b[0m   downsampled_image\u001b[39m.\u001b[39;49msave(output_path)\n\u001b[0;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m   \u001b[39m# Construct the full path to the image file\u001b[39;00m\n\u001b[0;32m     50\u001b[0m   image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, filename)\n","File \u001b[1;32mc:\\Users\\a829727\\Anaconda3\\envs\\image_up\\lib\\site-packages\\PIL\\Image.py:2431\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2428\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2431\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[0;32m   2432\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   2433\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n","File \u001b[1;32mc:\\Users\\a829727\\Anaconda3\\envs\\image_up\\lib\\site-packages\\PIL\\JpegImagePlugin.py:806\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[39m# The EXIF info needs to be written as one block, + APP1, + one spare byte.\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39m# Ensure that our buffer is big enough. Same with the icc_profile block.\u001b[39;00m\n\u001b[0;32m    804\u001b[0m bufsize \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(ImageFile\u001b[39m.\u001b[39mMAXBLOCK, bufsize, \u001b[39mlen\u001b[39m(exif) \u001b[39m+\u001b[39m \u001b[39m5\u001b[39m, \u001b[39mlen\u001b[39m(extra) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 806\u001b[0m ImageFile\u001b[39m.\u001b[39;49m_save(im, fp, [(\u001b[39m\"\u001b[39;49m\u001b[39mjpeg\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)], bufsize)\n","File \u001b[1;32mc:\\Users\\a829727\\Anaconda3\\envs\\image_up\\lib\\site-packages\\PIL\\ImageFile.py:520\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    518\u001b[0m     fh \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mfileno()\n\u001b[0;32m    519\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n\u001b[1;32m--> 520\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    522\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[39mNone\u001b[39;00m, exc)\n","File \u001b[1;32mc:\\Users\\a829727\\Anaconda3\\envs\\image_up\\lib\\site-packages\\PIL\\ImageFile.py:547\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    544\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    545\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    546\u001b[0m         \u001b[39m# slight speedup: compress to real file object\u001b[39;00m\n\u001b[1;32m--> 547\u001b[0m         s \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode_to_file(fh, bufsize)\n\u001b[0;32m    548\u001b[0m \u001b[39mif\u001b[39;00m s \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    549\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mencoder error \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m when writing image file\u001b[39m\u001b[39m\"\u001b[39m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def downsample_image(image_path, output_size):\n","    # Open the image\n","    image = Image.open(image_path)\n","\n","    # Perform downsampling using bicubic interpolation\n","    downscaled_image = image.resize(output_size, resample=Image.BICUBIC)\n","\n","    return downscaled_image\n","\n","# Folder path containing the images\n","folder_path = \"./data/mensch\"\n","\n","# Defining folder for downscaled images serving for input for modelling (&upscaling)\n","output_folder_path = \"./data/mensch_low_res\"\n","\n","test_low_res_folder = \"./data/mensch_test_low_res\"\n","test_original = \"./data/mensch_test_original\"\n","\n","if not os.path.exists(output_folder_path):\n","    os.makedirs(output_folder_path)\n","if not os.path.exists(test_low_res_folder):\n","    os.makedirs(test_low_res_folder)\n","if not os.path.exists(test_original):\n","    os.makedirs(test_original)\n","\n","\n","# Output size for downsampling (by a factor of 3)\n","output_size = (60, 88)\n","\n","number_images = len(os.listdir(folder_path))\n","\n","# Iterate over the files in the folder\n","for number, filename in enumerate(os.listdir(folder_path)):\n","\n","    # Check if the file is an image (optional)\n","    if number<=0.8*number_images:\n","      # Construct the full path to the image file\n","      image_path = os.path.join(folder_path, filename)\n","\n","      # Apply downsampling to the image\n","      downsampled_image = downsample_image(image_path, output_size)\n","\n","      # Save the downscaled image\n","      output_filename = f\"downsampled_{filename}\"\n","      output_path = os.path.join(output_folder_path, output_filename)\n","      downsampled_image.save(output_path)\n","\n","    else:\n","      # Construct the full path to the image file\n","      image_path = os.path.join(folder_path, filename)\n","\n","      image = Image.open(image_path)\n","      image.save(os.path.join(test_original, filename))\n","\n","\n","      # Apply downsampling to the image\n","      downsampled_image = downsample_image(image_path, output_size)\n","\n","      # Save the downscaled image\n","      output_filename = f\"downsampled_{filename}\"\n","      output_path = os.path.join(test_low_res_folder, output_filename)\n","      downsampled_image.save(output_path)\n","\n","      os.remove(os.path.join(folder_path, filename))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VrjMPPdsa8zL"},"source":["### data loading (with augmentation)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"elapsed":358973,"status":"error","timestamp":1687428359694,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"Hmwoaegma5-b","outputId":"64f003ce-dd79-461d-ce76-d96e392d1236"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-7f469c149e40>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0maugmented_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0maugmented_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_color_jitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_color_jitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0maugmented_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhue_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rgb2hsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36m_rgb2hsv\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# Implementation is based on https://github.com/python-pillow/Pillow/blob/4174d4267616897df3746d315d5a2d0f82c656ee/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# src/libImaging/Convert.c#L330\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mmaxc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0mminc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# low_res_folder = \"./drive/MyDrive/dog_low_res\"\n","# high_res_folder = \"./drive/MyDrive/dog\"\n","\n","\n","# # === creating dataset with all images ===\n","# class CustomDataset(Dataset):\n","#     def __init__(self, low_res_folder, high_res_folder, transform=None):\n","#         self.low_res_images = sorted(os.listdir(low_res_folder))\n","#         self.high_res_images = sorted(os.listdir(high_res_folder))\n","#         self.transform = transform\n","\n","#     def __len__(self):\n","#         return len(self.low_res_images)\n","\n","#     def __getitem__(self, index):\n","#         low_res_image = Image.open(os.path.join(low_res_folder, self.low_res_images[index]))\n","#         high_res_image = Image.open(os.path.join(high_res_folder, self.high_res_images[index]))\n","\n","#         if self.transform is not None:\n","#             low_res_image = self.transform(low_res_image)\n","#             high_res_image = self.transform(high_res_image)\n","\n","#         return low_res_image, high_res_image\n","\n","# # transform to tensor\n","# base_transform = transforms.Compose([\n","#     transforms.ToTensor()\n","# ])\n","\n","# # original dataset\n","# dataset = CustomDataset(low_res_folder, high_res_folder, transform=base_transform)\n","\n","\n","\n","# # === Splitting into train and val sets ===\n","\n","# train_size = 0.8  # Proportion of data to be used for training\n","# dataset_size = len(dataset)\n","# split = int(train_size * dataset_size)\n","# train_indices = list(range(split))\n","# val_indices = list(range(split, dataset_size))\n","\n","# # Create train dataset as a subset of the combined dataset\n","# train_dataset = Subset(dataset, train_indices)\n","\n","# # Create val dataset as a subset of the combined dataset\n","# val_dataset = Subset(dataset, val_indices)\n","\n","\n","\n","# # === Normalization ===\n","# normalize = Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","\n","# train_dataset = [(normalize(image), normalize(target)) for image, target in train_dataset]\n","\n","# val_dataset = [(normalize(image), normalize(target)) for image, target in val_dataset]\n","\n","\n","\n","# # === train_data augmentation ===\n","\n","# # color jitter augmentation for training\n","# train_color_jitter = ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n","\n","# # augmentation factor\n","# augmentation_factor = 2\n","\n","# # augmented datasets with random color jitter\n","# augmented_datasets = []\n","# for _ in range(augmentation_factor):\n","#     augmented_dataset = []\n","#     for image, target in train_dataset:\n","#         augmented_dataset.append((train_color_jitter(image), train_color_jitter(target)))\n","#     augmented_datasets.append(augmented_dataset)\n","\n","# # combine original and augmented datasets\n","# combined_datasets = [train_dataset] + augmented_datasets\n","# combined_dataset = ConcatDataset(combined_datasets)\n","\n","\n","\n","# # === final data loaders ===\n","\n","# # Data loaders for train and val sets\n","# batch_size = 32\n","# train_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n","# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# # Number of samples in each set\n","# print(f\"Number of training samples originally: {len(train_dataset)}, now augmented to: {len(combined_dataset)}\")\n","# print(f\"Number of val samples: {len(val_dataset)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["low_res_folder = \"/kaggle/working/dog_low_res\"\n","high_res_folder = \"/kaggle/input/animal-faces/afhq/train/dog\"\n","\n","\n","# === creating dataset with all images ===\n","class CustomDataset(Dataset):\n","    def __init__(self, low_res_folder, high_res_folder, transform=None):\n","        self.low_res_folder = low_res_folder\n","        self.high_res_folder = high_res_folder\n","        self.low_res_images = sorted(os.listdir(low_res_folder))\n","        self.high_res_images = sorted(os.listdir(high_res_folder))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.low_res_images)\n","\n","    def __getitem__(self, index):\n","        low_res_image = Image.open(os.path.join(self.low_res_folder, self.low_res_images[index]))\n","        high_res_image = Image.open(os.path.join(self.high_res_folder, self.high_res_images[index]))\n","\n","        if self.transform is not None:\n","            low_res_image = self.transform(low_res_image)\n","            high_res_image = self.transform(high_res_image)\n","\n","        return low_res_image, high_res_image\n","\n","# transform to tensor & normalize\n","normalize = Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","\n","base_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    normalize\n","])\n","\n","# original dataset\n","dataset = CustomDataset(low_res_folder, high_res_folder, transform=base_transform)\n","\n","\n","\n","# === Splitting into train and val sets ===\n","\n","train_size = 0.8  # Proportion of data to be used for training\n","dataset_size = len(dataset)\n","split = int(train_size * dataset_size)\n","train_indices = list(range(split))\n","val_indices = list(range(split, dataset_size))\n","\n","# Create train dataset as a subset of the combined dataset\n","train_dataset = Subset(dataset, train_indices)\n","\n","# Create val dataset as a subset of the combined dataset\n","val_dataset = Subset(dataset, val_indices)\n","\n","\n","\n","# === final data loaders ===\n","\n","# Data loaders for train and val sets\n","batch_size = 128\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Number of samples in each set\n","print(f\"Number of training samples originally: {len(train_dataset)}\")\n","print(f\"Number of val samples: {len(val_dataset)}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eRnCPEdja_mT"},"source":["### SRCNN"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":303652,"status":"ok","timestamp":1687426760337,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"0gzlOYjOZVy8","outputId":"985020d4-02da-4d2d-fa4e-06dbf6ef8801"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Loss: 0.1283, PSNR: 8.916304823903484\n","Loss (validation): 0.1736, PSNR (validation): 7.604106464489315\n","Epoch [2/10], Loss: 0.1220, PSNR: 9.137848423008013\n","Loss (validation): 0.1720, PSNR (validation): 7.645366720205501\n","Epoch [3/10], Loss: 0.0960, PSNR: 10.175107758089872\n","Loss (validation): 0.1720, PSNR (validation): 7.644895610697584\n","Epoch [4/10], Loss: 0.0786, PSNR: 11.04723886717254\n","Loss (validation): 0.1713, PSNR (validation): 7.663384318469921\n","Epoch [5/10], Loss: 0.0614, PSNR: 12.117327232179981\n","Loss (validation): 0.1710, PSNR (validation): 7.6689281909895435\n","Epoch [6/10], Loss: 0.1112, PSNR: 9.537440829689723\n","Loss (validation): 0.1712, PSNR (validation): 7.6646840091797594\n","Epoch [7/10], Loss: 0.1348, PSNR: 8.703944118333364\n","Loss (validation): 0.1722, PSNR (validation): 7.63884046002568\n","Epoch [8/10], Loss: 0.0904, PSNR: 10.438977789088515\n","Loss (validation): 0.1715, PSNR (validation): 7.657047145614541\n","Epoch [9/10], Loss: 0.0899, PSNR: 10.462438624621978\n","Loss (validation): 0.1710, PSNR (validation): 7.670334757039252\n","Epoch [10/10], Loss: 0.0677, PSNR: 11.693491150804247\n","Loss (validation): 0.1708, PSNR (validation): 7.675813618203781\n"]}],"source":["# SRCNN model\n","class SRCNN(nn.Module):\n","    def __init__(self):\n","        super(SRCNN, self).__init__()\n","        self.interpolation = nn.Upsample(scale_factor=4, mode='bicubic')\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0)\n","        self.relu2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, stride=1, padding=2)\n","        self.relu3 = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.interpolation(x)\n","        x = self.relu1(self.conv1(x))\n","        x = self.relu2(self.conv2(x))\n","        x = self.relu3(self.conv3(x))\n","        return x\n","\n","# Training hardware\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# instance of the CNN model\n","model = SRCNN().to(device)\n","\n","# hyperparameters\n","learning_rate = 0.001\n","num_epochs = 10\n","\n","# loss function and optimizer\n","criterion = nn.MSELoss() # note: standard MSE is used, PSNR normally not used for training (just as metric at the end)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training process\n","for epoch in range(num_epochs):\n","    for input_data, desired_data in train_loader:\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Print training loss per epoch\n","    psnr = 10 * math.log10(1 / loss.item())\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, PSNR: {psnr}\")\n","\n","    # Validating the model (on validation data)\n","    for input_data, desired_data in val_loader:\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","\n","    # Print training loss per epoch\n","    psnr = 10 * math.log10(1 / loss.item())\n","\n","    print(f\"Loss (validation): {loss.item():.4f}, PSNR (validation): {psnr}\")\n","\n","# saving the model\n","torch.save(model.state_dict(), \"SRCNN.pth\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IIwglB9obBWf"},"source":["### FSRCNN"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241593,"status":"ok","timestamp":1687427178503,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"61XUL-tBbCVo","outputId":"9314a921-3999-427f-8208-8816e981bb22"},"outputs":[{"ename":"NameError","evalue":"name 'train_loader' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m# Training process\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mfor\u001b[39;00m input_data, desired_data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     46\u001b[0m         \u001b[39m# Move input and desired images to device\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         input_data \u001b[39m=\u001b[39m input_data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     48\u001b[0m         desired_data \u001b[39m=\u001b[39m desired_data\u001b[39m.\u001b[39mto(device)\n","\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"]}],"source":["class FSRCNN(nn.Module):\n","    def __init__(self, d=56, s=12, m=4):\n","        super(FSRCNN, self).__init__()\n","        # Feature Extraction\n","        self.conv1 = nn.Conv2d(3, d, kernel_size=5, padding=2)\n","        self.relu1 = nn.PReLU(d)\n","        # Shrinking\n","        self.conv2 = nn.Conv2d(d, s, kernel_size=1)\n","        self.relu2 = nn.PReLU(s)\n","        # Non-linear Mapping\n","        self.mapping = nn.Sequential(*[nn.Sequential(\n","            nn.Conv2d(s, s, kernel_size=3, padding=1),\n","            nn.PReLU(s)\n","        ) for _ in range(m)])\n","        # Expanding\n","        self.conv3 = nn.Conv2d(s, d, kernel_size=1)\n","        self.relu3 = nn.PReLU(d)\n","        # Deconvolution\n","        self.deconv = nn.ConvTranspose2d(d, 3, kernel_size=9, stride=5, padding=4, output_padding=4)\n","\n","    def forward(self, x):\n","        x = self.relu1(self.conv1(x))\n","        x = self.relu2(self.conv2(x))\n","        x = self.mapping(x)\n","        x = self.relu3(self.conv3(x))\n","        x = self.deconv(x)\n","        return x\n","\n","# Training hardware\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# instance of the CNN model\n","model = FSRCNN().to(device)\n","\n","# hyperparameters\n","learning_rate = 0.001\n","num_epochs = 40\n","\n","# loss function and optimizer\n","criterion = nn.MSELoss() # note: standard MSE is used, PSNR normally not used for training (just as metric at the end)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training process\n","for epoch in range(num_epochs):\n","    for input_data, desired_data in train_loader:\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Print training loss per epoch\n","    psnr = 10 * math.log10(1 / loss.item())\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, PSNR: {psnr}\")\n","\n","    # Validating the model (on validation data)\n","    val_loss = 0\n","    number_batches = 0\n","    for input_data, desired_data in val_loader:\n","        number_batches += 1\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","        val_loss += loss.item()\n","    \n","    val_loss_avg = val_loss / number_batches   \n","\n","    # Print training loss per epoch\n","    psnr = 10 * math.log10(1 / val_loss_avg)\n","\n","    print(f\"Loss (validation): {val_loss_avg:.4f}, PSNR (validation): {psnr}\")\n","\n","\n","# saving the model\n","torch.save(model.state_dict(), \"FSRCNN.pth\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yrLNqyDyJt2P"},"source":["### Bicubic"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1074,"status":"ok","timestamp":1687427876086,"user":{"displayName":"Marcus","userId":"05894618993451037890"},"user_tz":-120},"id":"An0Ri4gtJZbC","outputId":"b5d806d6-c5f7-4dbe-f07a-69012eb94eef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss (validation): 0.0114, PSNR (validation): 19.41890646298794\n"]}],"source":["class bicubic(nn.Module):\n","    def __init__(self):\n","        super(bicubic, self).__init__()\n","        self.interpolation = nn.Upsample(scale_factor=4, mode='bicubic')\n","\n","    def forward(self, x):\n","        x = self.interpolation(x)\n","        return x\n","\n","# Training hardware\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# instance of the CNN model\n","model = bicubic().to(device)\n","\n","# loss function and optimizer\n","criterion = nn.MSELoss() # note: standard MSE is used, PSNR normally not used for training (just as metric at the end)\n","\n","# Validating the model (on validation data)\n","for input_data, desired_data in val_loader:\n","    # Move input and desired images to device\n","    input_data = input_data.to(device)\n","    desired_data = desired_data.to(device)\n","\n","    # Forward pass\n","    output_images = model(input_data)\n","\n","    # Calculate loss\n","    loss = criterion(output_images, desired_data)\n","\n","# Print training loss per epoch\n","psnr = 10 * math.log10(1 / loss.item())\n","\n","print(f\"Loss (validation): {loss.item():.4f}, PSNR (validation): {psnr}\")\n","\n","\n","# saving the model\n","torch.save(model.state_dict(), \"bicubic.pth\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5s9yDnLZLDk5"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOZYggG0vihTHkmSNt2+iCH","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
