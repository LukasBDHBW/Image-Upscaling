{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:40:17.144929Z","iopub.status.busy":"2023-06-24T17:40:17.144443Z","iopub.status.idle":"2023-06-24T17:40:20.223614Z","shell.execute_reply":"2023-06-24T17:40:20.222328Z","shell.execute_reply.started":"2023-06-24T17:40:17.144893Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import os\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.transforms import Normalize\n","import math"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### create low res copy of all test images"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:40:20.225964Z","iopub.status.busy":"2023-06-24T17:40:20.225366Z","iopub.status.idle":"2023-06-24T17:40:25.807387Z","shell.execute_reply":"2023-06-24T17:40:25.806228Z","shell.execute_reply.started":"2023-06-24T17:40:20.225933Z"},"trusted":true},"outputs":[],"source":["def downsample_image(image_path, output_size):\n","    # Open the image\n","    image = Image.open(image_path)\n","\n","    # Perform downsampling using bicubic interpolation\n","    downscaled_image = image.resize(output_size, resample=Image.BICUBIC)\n","\n","    return downscaled_image\n","\n","# Folder path containing the images\n","folder_path = \"/kaggle/input/animal-faces/afhq/val/dog\"\n","\n","# Defining folder for downscaled images serving for input for modelling (&upscaling)\n","output_folder_path = \"/kaggle/working/dog_low_res\"\n","if not os.path.exists(output_folder_path):\n","    os.makedirs(output_folder_path)\n","\n","\n","# Output size for downsampling (by a factor of 3)\n","output_size = (128, 128)\n","\n","# Iterate over the files in the folder\n","for number, filename in enumerate(os.listdir(folder_path)):\n","\n","  # Construct the full path to the image file\n","  image_path = os.path.join(folder_path, filename)\n","\n","  # Apply downsampling to the image\n","  downsampled_image = downsample_image(image_path, output_size)\n","\n","  # Save the downscaled image\n","  output_filename = f\"downsampled_{filename}\"\n","  output_path = os.path.join(output_folder_path, output_filename)\n","  downsampled_image.save(output_path)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### using same data loaders built for training process"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:40:25.810679Z","iopub.status.busy":"2023-06-24T17:40:25.808840Z","iopub.status.idle":"2023-06-24T17:40:25.822094Z","shell.execute_reply":"2023-06-24T17:40:25.820524Z","shell.execute_reply.started":"2023-06-24T17:40:25.810649Z"},"trusted":true},"outputs":[],"source":["low_res_folder = \"./data/Human/Humans_final_200_test\"\n","high_res_folder = \"./data/Human/Humans_final_600_test\"\n","\n","\n","# === creating dataset with all images ===\n","class CustomDataset(Dataset):\n","    def __init__(self, low_res_folder, high_res_folder, transform=None):\n","        self.low_res_folder = low_res_folder\n","        self.high_res_folder = high_res_folder\n","        self.low_res_images = sorted(os.listdir(low_res_folder))\n","        self.high_res_images = sorted(os.listdir(high_res_folder))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.low_res_images)\n","\n","    def __getitem__(self, index):\n","        low_res_image = Image.open(os.path.join(self.low_res_folder, self.low_res_images[index]))\n","        high_res_image = Image.open(os.path.join(self.high_res_folder, self.high_res_images[index]))\n","\n","        if self.transform is not None:\n","            low_res_image = self.transform(low_res_image)\n","            high_res_image = self.transform(high_res_image)\n","\n","        return low_res_image, high_res_image\n","\n","# transform to tensor & normalize\n","normalize = Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","\n","base_transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","# original dataset\n","dataset = CustomDataset(low_res_folder, high_res_folder, transform=base_transform)\n","\n","batch_size = 64\n","test_loader = DataLoader(dataset, batch_size=batch_size)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### defining structure of all models (needed since saving a model only saves the weights)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:40:25.825401Z","iopub.status.busy":"2023-06-24T17:40:25.824611Z","iopub.status.idle":"2023-06-24T17:40:25.846270Z","shell.execute_reply":"2023-06-24T17:40:25.844931Z","shell.execute_reply.started":"2023-06-24T17:40:25.825362Z"},"trusted":true},"outputs":[],"source":["class bicubic(nn.Module):\n","    def __init__(self):\n","        super(bicubic, self).__init__()\n","        self.interpolation = nn.Upsample(scale_factor=4, mode='bicubic')\n","\n","    def forward(self, x):\n","        x = self.interpolation(x)\n","        return x\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:40:25.848267Z","iopub.status.busy":"2023-06-24T17:40:25.847825Z","iopub.status.idle":"2023-06-24T17:40:25.860145Z","shell.execute_reply":"2023-06-24T17:40:25.858725Z","shell.execute_reply.started":"2023-06-24T17:40:25.848228Z"},"trusted":true},"outputs":[],"source":["# SRCNN model\n","class SRCNN(nn.Module):\n","    def __init__(self):\n","        super(SRCNN, self).__init__()\n","        self.interpolation = nn.Upsample(scale_factor=4, mode='bicubic')\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0)\n","        self.relu2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, stride=1, padding=2)\n","        self.relu3 = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.interpolation(x)\n","        x = self.relu1(self.conv1(x))\n","        x = self.relu2(self.conv2(x))\n","        x = self.relu3(self.conv3(x))\n","        return x"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:40:25.861927Z","iopub.status.busy":"2023-06-24T17:40:25.861503Z","iopub.status.idle":"2023-06-24T17:40:25.880412Z","shell.execute_reply":"2023-06-24T17:40:25.878991Z","shell.execute_reply.started":"2023-06-24T17:40:25.861891Z"},"trusted":true},"outputs":[],"source":["# Load the trained model\n","class FSRCNN(nn.Module):\n","    def __init__(self, d=88, s=18, m=6):\n","        super(FSRCNN, self).__init__()\n","        # Feature Extraction\n","        self.conv1 = nn.Conv2d(3, d, kernel_size=5, padding=2)\n","        self.relu1 = nn.PReLU(d)\n","        # Shrinking\n","        self.conv2 = nn.Conv2d(d, s, kernel_size=1)\n","        self.relu2 = nn.PReLU(s)\n","        # Non-linear Mapping\n","        self.mapping = nn.Sequential(*[nn.Sequential(\n","            nn.Conv2d(s, s, kernel_size=3, padding=1),\n","            nn.PReLU(s)\n","        ) for _ in range(m)])\n","        # Expanding\n","        self.conv3 = nn.Conv2d(s, d, kernel_size=1)\n","        self.relu3 = nn.PReLU(d)\n","        # Deconvolution\n","        self.deconv = nn.ConvTranspose2d(d, 3, kernel_size=9, stride=4, padding=4, output_padding=3)\n","\n","    def forward(self, x):\n","        x = self.relu1(self.conv1(x))\n","        x = self.relu2(self.conv2(x))\n","        x = self.mapping(x)\n","        x = self.relu3(self.conv3(x))\n","        x = self.deconv(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ESPCN(nn.Module):\n","    def __init__(self, upscale_factor=4, num_channels=3):\n","        super(ESPCN, self).__init__()\n","        self.upscale_factor = upscale_factor\n","        \n","        # Feature Extraction\n","        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=5, padding=2)\n","        self.relu1 = nn.ReLU()\n","        \n","        # Sub-Pixel Convolution\n","        self.conv2 = nn.Conv2d(64, num_channels * upscale_factor ** 2, kernel_size=3, padding=1)\n","        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n","        self.relu2 = nn.ReLU()\n","    \n","    def forward(self, x):\n","        x = self.relu1(self.conv1(x))\n","        x = self.pixel_shuffle(self.conv2(x))\n","        x = self.relu2(x)\n","        return x\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### loading a saved model (only run the cell of one modell at a time here)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:34:58.777948Z","iopub.status.busy":"2023-06-24T17:34:58.777592Z","iopub.status.idle":"2023-06-24T17:34:58.793280Z","shell.execute_reply":"2023-06-24T17:34:58.792358Z","shell.execute_reply.started":"2023-06-24T17:34:58.777922Z"},"trusted":true},"outputs":[{"data":{"text/plain":["bicubic(\n","  (interpolation): Upsample(scale_factor=4.0, mode='bicubic')\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model = bicubic()\n","model.load_state_dict(torch.load(\"/kaggle/input/bicubic/bicubic.pth\", map_location=torch.device('cpu')))\n","model.eval()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:40:25.882391Z","iopub.status.busy":"2023-06-24T17:40:25.882083Z","iopub.status.idle":"2023-06-24T17:40:25.965473Z","shell.execute_reply":"2023-06-24T17:40:25.964138Z","shell.execute_reply.started":"2023-06-24T17:40:25.882365Z"},"trusted":true},"outputs":[{"data":{"text/plain":["FSRCNN(\n","  (conv1): Conv2d(3, 88, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (relu1): PReLU(num_parameters=88)\n","  (conv2): Conv2d(88, 18, kernel_size=(1, 1), stride=(1, 1))\n","  (relu2): PReLU(num_parameters=18)\n","  (mapping): Sequential(\n","    (0): Sequential(\n","      (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): PReLU(num_parameters=18)\n","    )\n","    (1): Sequential(\n","      (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): PReLU(num_parameters=18)\n","    )\n","    (2): Sequential(\n","      (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): PReLU(num_parameters=18)\n","    )\n","    (3): Sequential(\n","      (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): PReLU(num_parameters=18)\n","    )\n","    (4): Sequential(\n","      (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): PReLU(num_parameters=18)\n","    )\n","    (5): Sequential(\n","      (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): PReLU(num_parameters=18)\n","    )\n","  )\n","  (conv3): Conv2d(18, 88, kernel_size=(1, 1), stride=(1, 1))\n","  (relu3): PReLU(num_parameters=88)\n","  (deconv): ConvTranspose2d(88, 3, kernel_size=(9, 9), stride=(4, 4), padding=(4, 4), output_padding=(3, 3))\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["model = FSRCNN()\n","model.load_state_dict(torch.load(\"./Modelle/Human/FSRCNN_20.pth\", map_location=torch.device('cpu')))\n","model.eval()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### test the model on a single defined image"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:33:12.798010Z","iopub.status.busy":"2023-06-24T17:33:12.797606Z","iopub.status.idle":"2023-06-24T17:33:12.998599Z","shell.execute_reply":"2023-06-24T17:33:12.997647Z","shell.execute_reply.started":"2023-06-24T17:33:12.797944Z"},"trusted":true},"outputs":[],"source":["# Preprocess the test image\n","path = \"./data/Human/Humans_final_200_test/\"\n","filename = \"downsampled_downsampled_1 (6898).jpg\"\n","test_image_path = path+filename\n","test_image = Image.open(test_image_path)\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","test_image = transform(test_image).unsqueeze(0)\n","\n","# Apply the model to the test image\n","with torch.no_grad():\n","    output_image = model(test_image)\n","\n","# Convert the output tensor to an image\n","output_image = output_image.squeeze(0)\n","output_image = transforms.ToPILImage()(output_image)\n","\n","# Save the output image\n","output_image.save(f\"./data/Human/Up/upscaled_{filename}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### apply the model to all the tesing data (saves upscaled images and calculates PSNR)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:42:45.775325Z","iopub.status.busy":"2023-06-24T17:42:45.774989Z","iopub.status.idle":"2023-06-24T17:43:13.303618Z","shell.execute_reply":"2023-06-24T17:43:13.302309Z","shell.execute_reply.started":"2023-06-24T17:42:45.775299Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 0.0020, PSNR: 26.887029451771824\n"]}],"source":["test_loss = 0\n","number_batches = 0\n","criterion = nn.MSELoss()\n","device = torch.device(\"cpu\")\n","\n","img_comparison_folder = \"/kaggle/working/img_comparison\"\n","\n","if not os.path.exists(img_comparison_folder):\n","    os.makedirs(img_comparison_folder)\n","\n","for input_data, desired_data in test_loader:\n","    with torch.no_grad():\n","        number_batches += 1\n","        if not os.path.exists(f\"/kaggle/working/img_comparison/batch_{number_batches}\"):\n","            os.makedirs(f\"/kaggle/working/img_comparison/batch_{number_batches}\")\n","\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","        test_loss += loss.item()\n","        \n","        # saving the images\n","        for number, output_image in enumerate(output_images):\n","            output_image = output_image.squeeze(0)\n","            output_image = transforms.ToPILImage()(output_image)\n","            output_image.save(f\"/kaggle/working/img_comparison/batch_{number_batches}/{number}_upscaled.jpg\")\n","            \n","        # saving low res images from the same batch into the same folder\n","        for number, image in enumerate(input_data):\n","            image = image.squeeze(0)\n","            image = transforms.ToPILImage()(image)\n","            image.save(f\"/kaggle/working/img_comparison/batch_{number_batches}/{number}_input.jpg\")\n","            \n","        # saving original images from the same batch into the same folder\n","        for number, image in enumerate(desired_data):\n","            image = image.squeeze(0)\n","            image = transforms.ToPILImage()(image)\n","            image.save(f\"/kaggle/working/img_comparison/batch_{number_batches}/{number}_original.jpg\")\n","\n","test_loss_avg = test_loss / number_batches\n","\n","# Print training loss per epoch\n","psnr = 10 * math.log10(1 / test_loss_avg)\n","\n","print(f\"Loss: {test_loss_avg:.4f}, PSNR: {psnr}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### calculate testing PSNR only (no images will be saved)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T17:35:13.888621Z","iopub.status.busy":"2023-06-24T17:35:13.888051Z","iopub.status.idle":"2023-06-24T17:35:19.157510Z","shell.execute_reply":"2023-06-24T17:35:19.156606Z","shell.execute_reply.started":"2023-06-24T17:35:13.888589Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","Loss: 0.0020, PSNR: 27.07621104266254\n"]}],"source":["import time\n","test_loss = 0\n","number_batches = 0\n","criterion = nn.MSELoss()\n","device = torch.device(\"cpu\")\n","\n","start_time = time.time()\n","for input_data, desired_data in test_loader:\n","    with torch.no_grad():\n","        number_batches += 1\n","        print(number_batches)\n","\n","        # Move input and desired images to device\n","        input_data = input_data.to(device)\n","        desired_data = desired_data.to(device)\n","\n","        # Forward pass\n","        output_images = model(input_data)\n","\n","        # Calculate loss\n","        loss = criterion(output_images, desired_data)\n","        test_loss += loss.item()\n","\n","end_time = time.time()  \n","\n","elapsed_time = end_time - start_time  \n","print(f\"Duration (total): {elapsed_time} seconds, duration per file: {elapsed_time/len(dataset)} seconds\")\n","\n","test_loss_avg = test_loss / number_batches\n","\n","# Print training loss per epoch\n","psnr = 10 * math.log10(1 / test_loss_avg)\n","\n","print(f\"Loss: {test_loss_avg:.4f}, PSNR: {psnr}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
