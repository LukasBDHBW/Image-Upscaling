{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Notebook for hyperparameter tuning (using optuna package)\n","#### (all adapted to dog dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### optuna installation and imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8972,"status":"ok","timestamp":1687520229868,"user":{"displayName":"Lukas Gren","userId":"03474024525300725669"},"user_tz":-120},"id":"hNxmUS-zTCW7","outputId":"e895906c-b80c-4372-c8ef-06116905f788"},"outputs":[],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4749,"status":"ok","timestamp":1687520234560,"user":{"displayName":"Lukas Gren","userId":"03474024525300725669"},"user_tz":-120},"id":"pObkfu0NShIe"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.utils.prune as prune\n","from torch.utils.data import DataLoader\n","import optuna\n","from PIL import Image\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms, datasets\n","import os\n","from torch.utils.data import DataLoader, Dataset\n","import math\n","from torchvision.transforms import ColorJitter, Normalize\n","from torch.utils.data import ConcatDataset\n","from torch.utils.data import Subset\n","from tqdm import tqdm\n","import tqdm"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Creating PyTorch dataloaders for loading the data into the models later on"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":991877,"status":"ok","timestamp":1687521268391,"user":{"displayName":"Lukas Gren","userId":"03474024525300725669"},"user_tz":-120},"id":"xvmctmX5ShIl","outputId":"5af64b7b-2881-4875-c4b0-eda469b00c39"},"outputs":[],"source":["low_res_folder = \"/kaggle/working/dog_low_res\"\n","high_res_folder = \"/kaggle/input/animal-faces/afhq/train/dog\"\n","\n","\n","# === creating dataset with all images ===\n","class CustomDataset(Dataset):\n","    def __init__(self, low_res_folder, high_res_folder, transform=None):\n","        self.low_res_folder = low_res_folder\n","        self.high_res_folder = high_res_folder\n","        self.low_res_images = sorted(os.listdir(low_res_folder))\n","        self.high_res_images = sorted(os.listdir(high_res_folder))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.low_res_images)\n","\n","    def __getitem__(self, index):\n","        low_res_image = Image.open(os.path.join(self.low_res_folder, self.low_res_images[index]))\n","        high_res_image = Image.open(os.path.join(self.high_res_folder, self.high_res_images[index]))\n","\n","        if self.transform is not None:\n","            low_res_image = self.transform(low_res_image)\n","            high_res_image = self.transform(high_res_image)\n","\n","        return low_res_image, high_res_image\n","\n","# transform to tensor\n","base_transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","# original dataset\n","dataset = CustomDataset(low_res_folder, high_res_folder, transform=base_transform)\n","\n","\n","\n","# === Splitting into train and val sets ===\n","\n","train_size = 0.8  # Proportion of data to be used for training\n","dataset_size = len(dataset)\n","split = int(train_size * dataset_size)\n","train_indices = list(range(split))\n","val_indices = list(range(split, dataset_size))\n","\n","# Create train dataset as a subset of the combined dataset\n","train_dataset = Subset(dataset, train_indices)\n","\n","# Create val dataset as a subset of the combined dataset\n","val_dataset = Subset(dataset, val_indices)\n","\n","\n","\n","### --------------------------------------------------------------------------------- ###\n","#if you use the Human Data --->\n","\"\"\"# # === train_data augmentation ===\n","\n","# # color jitter augmentation for training\n","train_color_jitter = ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n","\n","# # augmentation factor\n","augmentation_factor = 1\n","\n","# # augmented datasets with random color jitter\n","augmented_datasets = []\n","for _ in tqdm(range(augmentation_factor)):\n","    augmented_dataset = []\n","    for image, target in train_dataset:\n","        augmented_dataset.append((train_color_jitter(image), train_color_jitter(target)))\n","    augmented_datasets.append(augmented_dataset)\n","\n","# # combine original and augmented datasets\n","combined_datasets = [train_dataset] + augmented_datasets\n","train_dataset = ConcatDataset(combined_datasets)\"\"\"\n","### --------------------------------------------------------------------------------- ###\n","\n","\n","\n","# === final data loaders ===\n","\n","# Data loaders for train and val sets\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Number of samples in each set\n","print(f\"Number of training samples: {len(train_dataset)}\")\n","print(f\"Number of val samples: {len(val_dataset)}\")\n","\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### tuning hyperparameters for all models\n","##### following below each model will have two code cells, one for defining the model itself and the other one for tuning its hyperparameters using optuna"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ELOdKUia_0Ey"},"source":["#### SRCNN"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1687521268393,"user":{"displayName":"Lukas Gren","userId":"03474024525300725669"},"user_tz":-120},"id":"uLRf1YnPlN5f"},"outputs":[],"source":["# SRCNN model\n","class SRCNN(nn.Module):\n","    def __init__(self):\n","        super(SRCNN, self).__init__()\n","        self.interpolation = nn.Upsample(scale_factor=4, mode='bicubic')\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0)\n","        self.relu2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, stride=1, padding=2)\n","        self.relu3 = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.interpolation(x)\n","        x = self.relu1(self.conv1(x))\n","        x = self.relu2(self.conv2(x))\n","        x = self.relu3(self.conv3(x))\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1085919,"status":"ok","timestamp":1687522374688,"user":{"displayName":"Lukas Gren","userId":"03474024525300725669"},"user_tz":-120},"id":"PsZ2pjiU_UF_","outputId":"c52d3f26-2ed4-4d96-dadd-9193fbe9553e"},"outputs":[],"source":["num_epochs = 10\n","def optuna_train(trial):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n","    model = SRCNN().to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    patience = 5\n","    best_loss = None\n","    best_epoch = None\n","\n","    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n","        model.train()\n","        train_loss = 0\n","        for input_data, desired_data in train_loader:\n","            input_data = input_data.to(device)\n","            desired_data = desired_data.to(device)\n","\n","            output_images = model(input_data)\n","            loss = criterion(output_images, desired_data)\n","            train_loss += loss.item()\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        average_train_loss = train_loss / len(train_loader)\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for input_data, desired_data in val_loader:\n","                input_data = input_data.to(device)\n","                desired_data = desired_data.to(device)\n","\n","                output_images = model(input_data)\n","                loss = criterion(output_images, desired_data)\n","                val_loss += loss.item()\n","\n","        average_val_loss = val_loss / len(val_loader)\n","\n","        psnr = 10 * math.log10(1 / average_val_loss)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}, PSNR: {psnr}\")\n","\n","        if best_loss is None or average_val_loss < best_loss:\n","            best_loss = average_val_loss\n","            best_epoch = epoch\n","        elif epoch - best_epoch > patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","    return best_loss\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(optuna_train, n_trials=5)\n","\n","print('Best trial:')\n","trial = study.best_trial\n","print(f\"  Value: {trial.value}\")\n","print(f\"  Params: \")\n","for key, value in trial.params.items():\n","    print(f\"    {key}: {value}\")\n","\n","df = study.trials_dataframe()\n","df.to_csv('study_results_SRCNN.csv', index=False)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9GX52v9G_Uj3"},"source":["### FSRCNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWpGU2DrShIr"},"outputs":[],"source":["# Load the trained model\n","class FSRCNN(nn.Module):\n","    def __init__(self, d=116, s=15, m=3):\n","        super(FSRCNN, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(3, d, kernel_size=5, padding=2)\n","        self.relu1 = nn.PReLU(d)\n","\n","        self.conv2 = nn.Conv2d(d, s, kernel_size=1)\n","        self.relu2 = nn.PReLU(s)\n","\n","        self.mapping = nn.Sequential(*[nn.Sequential(\n","            nn.Conv2d(s, s, kernel_size=3, padding=1),\n","            nn.PReLU(s)\n","        ) for _ in range(m)])\n","\n","        self.conv3 = nn.Conv2d(s, d, kernel_size=1)\n","        self.relu3 = nn.PReLU(d)\n","        \n","        # Deconvolution for upscaling to desired size\n","        self.deconv = nn.ConvTranspose2d(d, 3, kernel_size=9, stride=4, padding=4, output_padding=3)\n","\n","    def forward(self, x):\n","        x = self.relu1(self.conv1(x))\n","        x = self.relu2(self.conv2(x))\n","        x = self.mapping(x)\n","        x = self.relu3(self.conv3(x))\n","        x = self.deconv(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1274034,"status":"ok","timestamp":1687504628377,"user":{"displayName":"Lukas Gren","userId":"03474024525300725669"},"user_tz":-120},"id":"hkmyyLiwWuSy","outputId":"143d751a-dc05-4f47-d3f2-555dfbf0ec65"},"outputs":[],"source":["num_epochs = 10\n","def optuna_train(trial):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n","    d = trial.suggest_int('d', 32, 128)\n","    s = trial.suggest_int('s', 5, 20)\n","    m = trial.suggest_int('m', 2, 8)\n","    model = FSRCNN().to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    patience = 5\n","    best_loss = None\n","    best_epoch = None\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0\n","        for input_data, desired_data in train_loader:\n","            input_data = input_data.to(device)\n","            desired_data = desired_data.to(device)\n","\n","            output_images = model(input_data)\n","            loss = criterion(output_images, desired_data)\n","            train_loss += loss.item()\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        average_train_loss = train_loss / len(train_loader)\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for input_data, desired_data in val_loader:\n","                input_data = input_data.to(device)\n","                desired_data = desired_data.to(device)\n","\n","                output_images = model(input_data)\n","                loss = criterion(output_images, desired_data)\n","                val_loss += loss.item()\n","\n","        average_val_loss = val_loss / len(val_loader)\n","\n","        psnr = 10 * math.log10(1 / average_val_loss)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}, PSNR: {psnr}\")\n","\n","        if best_loss is None or average_val_loss < best_loss:\n","            best_loss = average_val_loss\n","            best_epoch = epoch\n","        elif epoch - best_epoch > patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","    return best_loss\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(optuna_train, n_trials=25)\n","\n","print('Best trial:')\n","trial = study.best_trial\n","print(f\"  Value: {trial.value}\")\n","print(f\"  Params: \")\n","for key, value in trial.params.items():\n","    print(f\"    {key}: {value}\")\n","\n","df = study.trials_dataframe()\n","df.to_csv('/study_results_FSRCNN.csv', index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ESPCN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ESPCN(nn.Module):\n","    def __init__(self, upscale_factor=4, num_channels=3):\n","        super(ESPCN, self).__init__()\n","        self.upscale_factor = upscale_factor\n","        \n","        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=5, padding=2)\n","        self.relu1 = nn.ReLU()\n","        \n","        # Subpixel convolution with pixle shuffle for upscaling to desired size\n","        self.conv2 = nn.Conv2d(64, num_channels * upscale_factor ** 2, kernel_size=3, padding=1)\n","        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n","        self.relu2 = nn.ReLU()\n","    \n","    def forward(self, x):\n","        x = self.relu1(self.conv1(x))\n","        x = self.pixel_shuffle(self.conv2(x))\n","        x = self.relu2(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_epochs = 20\n","def optuna_train(trial):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    learning_rate = trial.suggest_loguniform('lr', 1e-4, 5e-4)\n","    model = ESPCN().to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    patience = 5\n","    best_loss = None\n","    best_epoch = None\n","\n","    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n","        model.train()\n","        train_loss = 0\n","        for input_data, desired_data in train_loader:\n","            input_data = input_data.to(device)\n","            desired_data = desired_data.to(device)\n","\n","            output_images = model(input_data)\n","            loss = criterion(output_images, desired_data)\n","            train_loss += loss.item()\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        average_train_loss = train_loss / len(train_loader)\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for input_data, desired_data in val_loader:\n","                input_data = input_data.to(device)\n","                desired_data = desired_data.to(device)\n","\n","                output_images = model(input_data)\n","                loss = criterion(output_images, desired_data)\n","                val_loss += loss.item()\n","\n","        average_val_loss = val_loss / len(val_loader)\n","\n","        psnr = 10 * math.log10(1 / average_val_loss)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}, PSNR: {psnr}\")\n","\n","        if best_loss is None or average_val_loss < best_loss:\n","            best_loss = average_val_loss\n","            best_epoch = epoch\n","        elif epoch - best_epoch > patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","    return best_loss\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(optuna_train, n_trials=2)\n","\n","print('Best trial:')\n","trial = study.best_trial\n","print(f\"  Value: {trial.value}\")\n","print(f\"  Params: \")\n","for key, value in trial.params.items():\n","    print(f\"    {key}: {value}\")\n","\n","df = study.trials_dataframe()\n","df.to_csv('study_results_ESPCN.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
